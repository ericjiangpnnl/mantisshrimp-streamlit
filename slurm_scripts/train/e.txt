/tmp/slurmd/job5754289/slurm_script: line 35: limit: command not found
Now you should run one of the following depending on your shell
source /share/apps/python/miniconda3.9/etc/profile.d/conda.sh
source /share/apps/python/miniconda3.9/etc/profile.d/conda.csh
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./train_V1p3.py
  min_nodes        : 8
  max_nodes        : 8
  nproc_per_node   : 2
  run_id           : 5754289
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.120.88:50797
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 2
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./train_V1p3.py
  min_nodes        : 8
  max_nodes        : 8
  nproc_per_node   : 2
  run_id           : 5754289
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.120.88:50797
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 2
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./train_V1p3.py
  min_nodes        : 8
  max_nodes        : 8
  nproc_per_node   : 2
  run_id           : 5754289
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.120.88:50797
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 2
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./train_V1p3.py
  min_nodes        : 8
  max_nodes        : 8
  nproc_per_node   : 2
  run_id           : 5754289
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.120.88:50797
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 2
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:50797 (errno: 97 - Address family not supported by protocol).
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./train_V1p3.py
  min_nodes        : 8
  max_nodes        : 8
  nproc_per_node   : 2
  run_id           : 5754289
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.120.88:50797
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 2
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_iarb0g_m/5754289_8lkn0o6w
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_n78m9v65/5754289_gdry2wai
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./train_V1p3.py
  min_nodes        : 8
  max_nodes        : 8
  nproc_per_node   : 2
  run_id           : 5754289
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.120.88:50797
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 2
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./train_V1p3.py
  min_nodes        : 8
  max_nodes        : 8
  nproc_per_node   : 2
  run_id           : 5754289
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.120.88:50797
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 2
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_f5b8unsi/5754289_yjnao45q
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_gn4ibb4y/5754289_npmv6wdw
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : ./train_V1p3.py
  min_nodes        : 8
  max_nodes        : 8
  nproc_per_node   : 2
  run_id           : 5754289
  rdzv_backend     : c10d
  rdzv_endpoint    : 172.16.120.88:50797
  rdzv_configs     : {'timeout': 900}
  max_restarts     : 2
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_pqt_zqek/5754289_vtvubkns
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_jtre40bs/5754289_lx_c1myq
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
[W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [dl09.local]:50797 (errno: 97 - Address family not supported by protocol).
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_d9b1zdbm/5754289_5i_97f84
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_3h8yzj94/5754289_u5ro52oj
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=dl09.local
  master_port=43450
  group_rank=0
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=dl09.local
  master_port=43450
  group_rank=1
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[2, 3]
  global_ranks=[2, 3]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=dl09.local
  master_port=43450
  group_rank=6
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[12, 13]
  global_ranks=[12, 13]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=dl09.local
  master_port=43450
  group_rank=7
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[14, 15]
  global_ranks=[14, 15]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=dl09.local
  master_port=43450
  group_rank=3
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[6, 7]
  global_ranks=[6, 7]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=dl09.local
  master_port=43450
  group_rank=4
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[8, 9]
  global_ranks=[8, 9]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=dl09.local
  master_port=43450
  group_rank=2
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[4, 5]
  global_ranks=[4, 5]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=dl09.local
  master_port=43450
  group_rank=5
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[10, 11]
  global_ranks=[10, 11]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_iarb0g_m/5754289_8lkn0o6w/attempt_0/0/error.json
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_iarb0g_m/5754289_8lkn0o6w/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_gn4ibb4y/5754289_npmv6wdw/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_f5b8unsi/5754289_yjnao45q/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_jtre40bs/5754289_lx_c1myq/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_n78m9v65/5754289_gdry2wai/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_d9b1zdbm/5754289_5i_97f84/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_3h8yzj94/5754289_u5ro52oj/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_pqt_zqek/5754289_vtvubkns/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_gn4ibb4y/5754289_npmv6wdw/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_f5b8unsi/5754289_yjnao45q/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_jtre40bs/5754289_lx_c1myq/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_n78m9v65/5754289_gdry2wai/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_d9b1zdbm/5754289_5i_97f84/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_3h8yzj94/5754289_u5ro52oj/attempt_0/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_pqt_zqek/5754289_vtvubkns/attempt_0/1/error.json
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
    from mantis_shtimp import datasets
    from mantis_shtimp import datasets
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
ModuleNotFoundError: No module named 'mantis_shtimp'
ModuleNotFoundError: No module named 'mantis_shtimp'
ModuleNotFoundError: No module named 'mantis_shtimp'
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
    from mantis_shtimp import datasets
    from mantis_shtimp import datasets
    from mantis_shtimp import datasets
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
ModuleNotFoundError: No module named 'mantis_shtimp'
ModuleNotFoundError: No module named 'mantis_shtimp'
ModuleNotFoundError: No module named 'mantis_shtimp'
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
ModuleNotFoundError: No module named 'mantis_shtimp'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10114) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 6967) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 11825) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10585) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3388) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 16632) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 26365) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 24700) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 2/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=dl09.local
  master_port=38565
  group_rank=0
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=dl09.local
  master_port=38565
  group_rank=1
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[2, 3]
  global_ranks=[2, 3]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=dl09.local
  master_port=38565
  group_rank=6
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[12, 13]
  global_ranks=[12, 13]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=dl09.local
  master_port=38565
  group_rank=7
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[14, 15]
  global_ranks=[14, 15]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=dl09.local
  master_port=38565
  group_rank=3
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[6, 7]
  global_ranks=[6, 7]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=dl09.local
  master_port=38565
  group_rank=4
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[8, 9]
  global_ranks=[8, 9]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=dl09.local
  master_port=38565
  group_rank=2
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[4, 5]
  global_ranks=[4, 5]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=dl09.local
  master_port=38565
  group_rank=5
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[10, 11]
  global_ranks=[10, 11]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_iarb0g_m/5754289_8lkn0o6w/attempt_1/0/error.json
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_iarb0g_m/5754289_8lkn0o6w/attempt_1/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_gn4ibb4y/5754289_npmv6wdw/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_f5b8unsi/5754289_yjnao45q/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_jtre40bs/5754289_lx_c1myq/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_n78m9v65/5754289_gdry2wai/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_d9b1zdbm/5754289_5i_97f84/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_3h8yzj94/5754289_u5ro52oj/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_pqt_zqek/5754289_vtvubkns/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_gn4ibb4y/5754289_npmv6wdw/attempt_1/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_f5b8unsi/5754289_yjnao45q/attempt_1/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_jtre40bs/5754289_lx_c1myq/attempt_1/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_n78m9v65/5754289_gdry2wai/attempt_1/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_d9b1zdbm/5754289_5i_97f84/attempt_1/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_3h8yzj94/5754289_u5ro52oj/attempt_1/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_pqt_zqek/5754289_vtvubkns/attempt_1/1/error.json
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10156) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/2 attempts left; will restart worker group
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 26408) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 11870) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 24745) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10629) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3431) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 16673) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 7020) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 1/2 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=dl09.local
  master_port=39461
  group_rank=0
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=dl09.local
  master_port=39461
  group_rank=1
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[2, 3]
  global_ranks=[2, 3]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=dl09.local
  master_port=39461
  group_rank=6
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[12, 13]
  global_ranks=[12, 13]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=dl09.local
  master_port=39461
  group_rank=7
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[14, 15]
  global_ranks=[14, 15]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=dl09.local
  master_port=39461
  group_rank=3
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[6, 7]
  global_ranks=[6, 7]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=dl09.local
  master_port=39461
  group_rank=4
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[8, 9]
  global_ranks=[8, 9]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=dl09.local
  master_port=39461
  group_rank=2
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[4, 5]
  global_ranks=[4, 5]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=2
  master_addr=dl09.local
  master_port=39461
  group_rank=5
  group_world_size=8
  local_ranks=[0, 1]
  role_ranks=[10, 11]
  global_ranks=[10, 11]
  role_world_sizes=[16, 16]
  global_world_sizes=[16, 16]

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_iarb0g_m/5754289_8lkn0o6w/attempt_2/0/error.json
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.agent.server.local_elastic_agent:Environment variable 'TORCHELASTIC_ENABLE_FILE_TIMER' not found. Do not start FileTimerServer.
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_iarb0g_m/5754289_8lkn0o6w/attempt_2/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_gn4ibb4y/5754289_npmv6wdw/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_f5b8unsi/5754289_yjnao45q/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_jtre40bs/5754289_lx_c1myq/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_n78m9v65/5754289_gdry2wai/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_d9b1zdbm/5754289_5i_97f84/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_3h8yzj94/5754289_u5ro52oj/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_pqt_zqek/5754289_vtvubkns/attempt_2/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_gn4ibb4y/5754289_npmv6wdw/attempt_2/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_f5b8unsi/5754289_yjnao45q/attempt_2/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_jtre40bs/5754289_lx_c1myq/attempt_2/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_n78m9v65/5754289_gdry2wai/attempt_2/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_d9b1zdbm/5754289_5i_97f84/attempt_2/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_3h8yzj94/5754289_u5ro52oj/attempt_2/1/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_pqt_zqek/5754289_vtvubkns/attempt_2/1/error.json
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
Traceback (most recent call last):
ModuleNotFoundError: No module named 'mantis_shtimp'
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
ModuleNotFoundError: No module named 'mantis_shtimp'
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
Traceback (most recent call last):
  File "/rcfs/projects/mantis_shrimp/mantis_shrimp/NOTEBOOKS/./train_V1p3.py", line 5, in <module>
    from mantis_shtimp import datasets
ModuleNotFoundError: No module named 'mantis_shtimp'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10174) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (WorkerState.FAILED). Waiting 300 seconds for other agents to finish
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 16691) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 26426) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 7038) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 11897) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 24763) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 10655) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3449) of binary: /people/enge625/.conda/envs/torch2.0/bin/python
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (WorkerState.FAILED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (WorkerState.FAILED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (WorkerState.FAILED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (WorkerState.FAILED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (WorkerState.FAILED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (WorkerState.FAILED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (WorkerState.FAILED). Waiting 300 seconds for other agents to finish
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0031175613403320312 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.003365755081176758 seconds
srun: error: dl13: task 3: Exited with exit code 1
srun: error: dl10: task 1: Exited with exit code 1
srun: error: dl19: task 7: Exited with exit code 1
srun: error: dl18: task 6: Exited with exit code 1
srun: error: dl17: task 5: Exited with exit code 1
srun: error: dl12: task 2: Exited with exit code 1
srun: error: dl15: task 4: Exited with exit code 1
srun: error: dl09: task 0: Exited with exit code 1
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.003459453582763672 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0036420822143554688 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.004024982452392578 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.003910064697265625 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.004006147384643555 seconds
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.005269527435302734 seconds
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 2 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 12 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 14 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 8 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 4 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 0 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
Traceback (most recent call last):
  File "/people/enge625/.conda/envs/torch2.0/bin/torchrun", line 33, in <module>
Traceback (most recent call last):
Traceback (most recent call last):
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 6 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
Traceback (most recent call last):
Traceback (most recent call last):
INFO:torch.distributed.elastic.multiprocessing.errors:local_rank 10 FAILED with no error file. Decorate your entrypoint fn with @record for traceback info. See: https://pytorch.org/docs/stable/elastic/errors.html
Traceback (most recent call last):
  File "/people/enge625/.conda/envs/torch2.0/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
  File "/people/enge625/.conda/envs/torch2.0/bin/torchrun", line 33, in <module>
  File "/people/enge625/.conda/envs/torch2.0/bin/torchrun", line 33, in <module>
  File "/people/enge625/.conda/envs/torch2.0/bin/torchrun", line 33, in <module>
  File "/people/enge625/.conda/envs/torch2.0/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
Traceback (most recent call last):
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
Traceback (most recent call last):
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/enge625/.conda/envs/torch2.0/bin/torchrun", line 33, in <module>
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/enge625/.conda/envs/torch2.0/bin/torchrun", line 33, in <module>
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    sys.exit(load_entry_point('torch==2.0.1', 'console_scripts', 'torchrun')())
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
    return f(*args, **kwargs)
    return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    return f(*args, **kwargs)
    return f(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 794, in main
           ^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
           ^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 794, in main
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 794, in main
    return f(*args, **kwargs)
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 794, in main
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 794, in main
    return f(*args, **kwargs)
    run(args)
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 784, in run
    run(args)
    run(args)
           ^^^^^^^^^^^^^^^^^^
    run(args)
    run(args)
           ^^^^^^^^^^^^^^^^^^
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 784, in run
    elastic_launch(
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 784, in run
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 784, in run
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 794, in main
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 784, in run
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 784, in run
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 794, in main
    elastic_launch(
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    elastic_launch(
    elastic_launch(
    run(args)
    elastic_launch(
    elastic_launch(
    run(args)
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 784, in run
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/run.py", line 784, in run
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    return launch_agent(self._config, self._entrypoint, list(args))
    return launch_agent(self._config, self._entrypoint, list(args))
    elastic_launch(
    return launch_agent(self._config, self._entrypoint, list(args))
    return launch_agent(self._config, self._entrypoint, list(args))
    elastic_launch(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 254, in launch_agent
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 254, in launch_agent
    raise ChildFailedError(
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 254, in launch_agent
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 254, in launch_agent
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 254, in launch_agent
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 254, in launch_agent
    return launch_agent(self._config, self._entrypoint, list(args))
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./train_V1p3.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-05-25_14:00:41
  host      : dl10.local
  rank      : 3 (local_rank: 1)
  exitcode  : 1 (pid: 16692)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-05-25_14:00:41
  host      : dl10.local
  rank      : 2 (local_rank: 0)
  exitcode  : 1 (pid: 16691)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
    raise ChildFailedError(
    raise ChildFailedError(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    raise ChildFailedError(
    raise ChildFailedError(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./train_V1p3.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-05-25_14:00:41
  host      : dl09.local
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 10175)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-05-25_14:00:41
  host      : dl09.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 10174)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./train_V1p3.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-05-25_14:00:41
  host      : dl18.local
  rank      : 13 (local_rank: 1)
  exitcode  : 1 (pid: 26427)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-05-25_14:00:41
  host      : dl18.local
  rank      : 12 (local_rank: 0)
  exitcode  : 1 (pid: 26426)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./train_V1p3.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-05-25_14:00:41
  host      : dl19.local
  rank      : 15 (local_rank: 1)
  exitcode  : 1 (pid: 7039)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-05-25_14:00:41
  host      : dl19.local
  rank      : 14 (local_rank: 0)
  exitcode  : 1 (pid: 7038)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 254, in launch_agent
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./train_V1p3.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-05-25_14:00:41
  host      : dl15.local
  rank      : 9 (local_rank: 1)
  exitcode  : 1 (pid: 24764)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-05-25_14:00:41
  host      : dl15.local
  rank      : 8 (local_rank: 0)
  exitcode  : 1 (pid: 24763)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./train_V1p3.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-05-25_14:00:41
  host      : dl12.local
  rank      : 5 (local_rank: 1)
  exitcode  : 1 (pid: 10656)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-05-25_14:00:41
  host      : dl12.local
  rank      : 4 (local_rank: 0)
  exitcode  : 1 (pid: 10655)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
  File "/people/enge625/.conda/envs/torch2.0/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 254, in launch_agent
    raise ChildFailedError(
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./train_V1p3.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-05-25_14:00:41
  host      : dl13.local
  rank      : 7 (local_rank: 1)
  exitcode  : 1 (pid: 11898)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-05-25_14:00:41
  host      : dl13.local
  rank      : 6 (local_rank: 0)
  exitcode  : 1 (pid: 11897)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./train_V1p3.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-05-25_14:00:41
  host      : dl17.local
  rank      : 11 (local_rank: 1)
  exitcode  : 1 (pid: 3450)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-05-25_14:00:41
  host      : dl17.local
  rank      : 10 (local_rank: 0)
  exitcode  : 1 (pid: 3449)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
