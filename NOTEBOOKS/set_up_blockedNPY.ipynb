{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "293ddbb7",
   "metadata": {},
   "source": [
    "# in this notebook, we are actually going to write the true HDF5 files we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a0bf94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from utils import get_photopath\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28f85cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = [os.path.exists(get_photopath(i)) for i in range(len(DF))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d488e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save('/rcfs/projects/mantis_shrimp/mantis_shrimp/data/exists_mask.npy',np.array(mask),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83aa1d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.load('/rcfs/projects/mantis_shrimp/mantis_shrimp/data/exists_mask.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b457c5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_164600/3335362331.py:11: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  DF = DF.drop('bestObjID',1)\n"
     ]
    }
   ],
   "source": [
    "csv_path='/rcfs/projects/mantis_shrimp/mantis_shrimp/data/spectroscopy/redshifts_withextinction.pkl'\n",
    "csv_path2='/rcfs/projects/mantis_shrimp/mantis_shrimp/data/redshifts_broken_beck/SDSS_MGS/MGS_qwsaz123.csv'\n",
    "csv_path3='/rcfs/projects/mantis_shrimp/mantis_shrimp/data/redshifts_broken_beck/WISE_PS1_STRM.csv'\n",
    "\n",
    "DF = pd.read_pickle(csv_path,)\n",
    "\n",
    "#next load SDSS_MGS csv, merge into\n",
    "DF_pas = pd.read_csv(csv_path2,usecols=['bestObjID','zphot'])\n",
    "DF_pas.drop_duplicates('bestObjID',inplace=True)\n",
    "DF = pd.merge(DF,DF_pas,'left',left_on='photoObjID_survey',right_on='bestObjID')\n",
    "DF = DF.drop('bestObjID',1)\n",
    "\n",
    "#now load WISE_PS1_STRM csv, merge into\n",
    "DF_wps = pd.read_csv(csv_path3,\n",
    "                     usecols=['dstArcSec',\n",
    "                              'cellDistance_Photoz',\n",
    "                              'z_phot0',\n",
    "                              'z_photErr',\n",
    "                              'prob_Galaxy',\n",
    "                              'photoObjID_survey'])\n",
    "DF = pd.merge(DF,DF_wps,how='left',on='photoObjID_survey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e07f2878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#43 different hdf5 files. \n",
    "feature_size = 2*(32*32*2) + (5*170*170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dac8a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_data(indices):\n",
    "    image_vectors = []\n",
    "    for j in tqdm(indices):\n",
    "        filename = get_photopath(j)\n",
    "        if not(mask[j]): #file doesnt exist\n",
    "            #we can do this now and then target these vectors later to modify.\n",
    "            image_vector = np.zeros(feature_size).astype(np.float32)\n",
    "            #Since hdf5 has compression, should maybe not cost ttoo much.\n",
    "        else: #file does exist\n",
    "            img = np.load(get_photopath(j),allow_pickle=True).item()\n",
    "\n",
    "            unwise = rearrange(img['unwise'],'f h w -> (f h w)')\n",
    "            galex = rearrange(img['galex'],'f h w -> (f h w)')\n",
    "            panstarrs = rearrange(img['panstarrs'],'f h w -> (f h w)')\n",
    "        \n",
    "            image_vector = np.concatenate([galex,panstarrs,unwise])\n",
    "            \n",
    "        image_vectors.append(image_vector)\n",
    "    #now we can place this into our hdf5 file\n",
    "    image_vectors = np.stack(image_vectors)\n",
    "    return image_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b55c4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Because I use a seed, this should allow me to always re-generate the mapping.\n",
    "# all_indices = np.arange(len(mask))\n",
    "\n",
    "# np.random.seed(2212024)\n",
    "# random.seed(2212024)\n",
    "\n",
    "# import random\n",
    "# random.shuffle(all_indices)\n",
    "\n",
    "# train_indices = all_indices[0:int(0.7*len(all_indices))]\n",
    "# val_indices = all_indices[int(0.7*len(all_indices)):int(0.8*len(all_indices))]\n",
    "# test_indices = all_indices[int(0.8*len(all_indices))::]\n",
    "\n",
    "# #now sort into 16 vectors of equal length.\n",
    "# train_indices = train_indices[0:16 * (len(train_indices)//16)].reshape(16,-1)\n",
    "# val_indices = val_indices[0:16* (len(val_indices)//16)].reshape(16,-1)\n",
    "# test_indices = test_indices[0:16 * (len(test_indices)//16)].reshape(16,-1)\n",
    "\n",
    "# np.save('/rcfs/projects/mantis_shrimp/mantis_shrimp/data/npy_blocks/train_indices.npy',train_indices,)\n",
    "# np.save('/rcfs/projects/mantis_shrimp/mantis_shrimp/data/npy_blocks/val_indices.npy',val_indices,)\n",
    "# np.save('/rcfs/projects/mantis_shrimp/mantis_shrimp/data/npy_blocks/test_indices.npy',test_indices,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80a0940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = np.load('/rcfs/projects/mantis_shrimp/mantis_shrimp/data/npy_blocks/train_indices.npy',)\n",
    "val_indices = np.load('/rcfs/projects/mantis_shrimp/mantis_shrimp/data/npy_blocks/val_indices.npy',)\n",
    "test_indices = np.load('/rcfs/projects/mantis_shrimp/mantis_shrimp/data/npy_blocks/test_indices.npy',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c73b6e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|██████████████████████▌                                                | 60217/189137 [5:28:38<11:43:35,  3.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#f.create_group('photometry')\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m load_image_data(train_indices[i]) \u001b[38;5;66;03m#return an image of size 100_000 x feature_size\u001b[39;00m\n\u001b[1;32m     10\u001b[0m np\u001b[38;5;241m.\u001b[39msave(filename,data)\n",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mload_image_data\u001b[0;34m(indices)\u001b[0m\n\u001b[1;32m      7\u001b[0m     image_vector \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(feature_size)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#Since hdf5 has compression, should maybe not cost ttoo much.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m#file does exist\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(get_photopath(j),allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     12\u001b[0m     unwise \u001b[38;5;241m=\u001b[39m rearrange(img[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munwise\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf h w -> (f h w)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m     galex \u001b[38;5;241m=\u001b[39m rearrange(img[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgalex\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf h w -> (f h w)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/torch2.0/lib/python3.11/site-packages/numpy/lib/npyio.py:412\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    410\u001b[0m _ZIP_SUFFIX \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPK\u001b[39m\u001b[38;5;130;01m\\x05\u001b[39;00m\u001b[38;5;130;01m\\x06\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# empty zip files start with this\u001b[39;00m\n\u001b[1;32m    411\u001b[0m N \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX)\n\u001b[0;32m--> 412\u001b[0m magic \u001b[38;5;241m=\u001b[39m fid\u001b[38;5;241m.\u001b[39mread(N)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# If the file size is less than N, we need to make sure not\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# to seek past the beginning of the file\u001b[39;00m\n\u001b[1;32m    415\u001b[0m fid\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mmin\u001b[39m(N, \u001b[38;5;28mlen\u001b[39m(magic)), \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# back-up\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(0,16):\n",
    "    filename = f'/rcfs/projects/mantis_shrimp/mantis_shrimp/data/npy_blocks/train/mantis_shrimp_{i}.npy'\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        continue\n",
    "        \n",
    "    #f.create_group('photometry')\n",
    "    data = load_image_data(train_indices[i]) #return an image of size 100_000 x feature_size\n",
    "\n",
    "    np.save(filename,data)\n",
    "    #My other dataset already shows how to extract the remainder of the metadata I need. \n",
    "    #I'm going to use that instead, and save train_indices, val_indices, and test_indices so that I can access them\n",
    "    #at any time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
